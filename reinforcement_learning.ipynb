{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "4gTYMFGYRo9h"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras import layers\n",
    "from scipy.signal import convolve2d\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p2TH0Lfi50ou",
    "outputId": "db99a8e2-c0e1-4a26-f024-43af41baf2ed"
   },
   "outputs": [],
   "source": [
    "tf.__version__\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0],True)\n",
    "tf.random.set_seed(314)\n",
    "np.random.seed(314)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wXWSY0x4fcq4"
   },
   "source": [
    "# Creating the game/env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "IYrCXEP9WQjR"
   },
   "outputs": [],
   "source": [
    "class connectMNK():\n",
    "    def __init__(self,m,n,k):\n",
    "        self.board = np.zeros((m,n), dtype = 'float32')\n",
    "        self.player = np.array(1,dtype = 'float32')\n",
    "        self.m = m\n",
    "        self.n = n\n",
    "        self.k = k\n",
    "        self.history = []\n",
    "\n",
    "    def reset(self):\n",
    "        self.board = np.zeros((self.m,self.n), dtype = 'float32')\n",
    "        self.player = np.array(1,dtype = 'float32')\n",
    "        self.history = []\n",
    "    \n",
    "    def get_input_planes(self):\n",
    "        '''\n",
    "        r = np.array([(self.board**2+self.board)/2,\n",
    "                      (self.board**2-self.board)/2,\n",
    "                      np.ones((self.m,self.n)) if self.player == 1 else np.zeros((self.m,self.n)),\n",
    "                      np.ones((self.m,self.n)) if self.player == -1 else np.zeros((self.m,self.n))])\n",
    "        '''\n",
    "        b = self.board*self.player\n",
    "        r = np.array([(b**2+b)/2,\n",
    "                      (b**2-b)/2])\n",
    "        return np.reshape(r,(2,self.m,self.n))\n",
    "    \n",
    "    def valid_actions(self):\n",
    "        idx = np.array(range(self.m*self.n))\n",
    "        return idx[self.board.flatten()==0]\n",
    "    \n",
    "    def mask_actions(self):\n",
    "        return (np.ones((self.m,self.n))-self.board**2).flatten()\n",
    "\n",
    "    def do_action(self,pos):\n",
    "        i = pos//self.m\n",
    "        j = pos%self.n\n",
    "        self.board[i][j] = self.player\n",
    "        self.player = self.player * (-1)\n",
    "        self.history.append(pos)\n",
    "        return\n",
    "        \n",
    "    def undo(self):\n",
    "        self.board[self.history[-1]] = 0\n",
    "        self.player = self.player * (-1)\n",
    "        self.history.pop()\n",
    "        return\n",
    "\n",
    "    def check_victory(self):\n",
    "        horizontal_kernel = np.ones((1,self.k))\n",
    "        vertical_kernel = np.transpose(horizontal_kernel)\n",
    "        diag1_kernel = np.eye(self.k, dtype=np.uint8)\n",
    "        diag2_kernel = np.fliplr(diag1_kernel)\n",
    "        detection_kernels = [horizontal_kernel, vertical_kernel, diag1_kernel, diag2_kernel]\n",
    "        \n",
    "        for kernel in detection_kernels:\n",
    "            conv = np.array(convolve2d(self.board, kernel, mode=\"valid\"))\n",
    "            \n",
    "            if (conv == self.k).any():\n",
    "                return True,1\n",
    "            elif (conv == -self.k).any():\n",
    "                return True,-1\n",
    "        \n",
    "        if np.sum(self.mask_actions()) == 0:\n",
    "            return True, 0\n",
    "        \n",
    "        return False,0\n",
    "    \n",
    "    def __repr__(self):\n",
    "        b = self.board.astype(\"int\")\n",
    "        s=[\" \",\"X\",\"O\"]\n",
    "        r = \"\"\n",
    "        for i in range(self.m):\n",
    "            for j in range(self.n):\n",
    "                r += s[b[i][j]]\n",
    "                if(j<self.n-1):\n",
    "                    r += \"|\"\n",
    "            if(i<self.m-1):\n",
    "                r +=\"\\n\"+\"-\"*(self.n*2-1)+\"\\n\"\n",
    "        return r\n",
    "    \n",
    "    def __str__(self):\n",
    "        return self.__repr__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For gomoku:\n",
    "#M = 15\n",
    "#N = 15\n",
    "#K = 5\n",
    "\n",
    "#For tic-tac-toe:\n",
    "M = 3\n",
    "N = 3\n",
    "K = 3\n",
    "\n",
    "game = connectMNK(M,N,K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(False, 0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "game.reset()\n",
    "moves = game.valid_actions()\n",
    "game.do_action(np.random.choice(moves))\n",
    "game.check_victory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " |X| \n",
       "-----\n",
       " | | \n",
       "-----\n",
       " | |O"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "done = 0\n",
    "game.reset()\n",
    "game.do_action(1)\n",
    "game.do_action(8)\n",
    "game"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combined Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combined_model(blocks=1):\n",
    "    input_shape = (2,M,N,)\n",
    "    input1 = layers.Input(input_shape)\n",
    "    \n",
    "    x = layers.Conv2D(128, (3, 3), strides=(1, 1), padding='same',\n",
    "                               use_bias=False, data_format=\"channels_first\")(input1)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.LeakyReLU()(x)\n",
    "    \n",
    "    for i in range(1,blocks):\n",
    "        x = layers.Conv2D(128, (3, 3), strides=(1, 1), padding='same',\n",
    "                                   use_bias=False, data_format=\"channels_first\")(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.LeakyReLU()(x)\n",
    "    \n",
    "    x = layers.Flatten()(x)\n",
    "    \n",
    "    probs = layers.Dense(M*N)(x)\n",
    "    probs = layers.Softmax()(probs)\n",
    "    \n",
    "    value = layers.Dense(1,activation=\"tanh\")(x)\n",
    "    \n",
    "    return tf.keras.Model(input1,[probs,value])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 2, 3, 3)]    0           []                               \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)                (None, 128, 3, 3)    2304        ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization (BatchNorm  (None, 128, 3, 3)   12          ['conv2d[0][0]']                 \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " leaky_re_lu (LeakyReLU)        (None, 128, 3, 3)    0           ['batch_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 1152)         0           ['leaky_re_lu[0][0]']            \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 9)            10377       ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      " softmax (Softmax)              (None, 9)            0           ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 1)            1153        ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 13,846\n",
      "Trainable params: 13,840\n",
      "Non-trainable params: 6\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = combined_model(1)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def model_wrap(x):\n",
    "    return model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MCTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ucb_score(parent, child):\n",
    "    \"\"\"\n",
    "    The score for an action that would transition between the parent and child.\n",
    "    \"\"\"\n",
    "    prior_score = child.prior * np.sqrt(parent.visit_count) / (child.visit_count + 1)\n",
    "    if child.visit_count > 0:\n",
    "        # The value of the child is from the perspective of the opposing player\n",
    "        value_score = -child.value()\n",
    "    else:\n",
    "        value_score = 0\n",
    "\n",
    "    return value_score + prior_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node():\n",
    "    def __init__(self,prior, state, player):\n",
    "        self.prior = prior\n",
    "        self.state = state\n",
    "        self.player = player\n",
    "        \n",
    "        self.value_sum = 0\n",
    "        self.visit_count = 0\n",
    "        self.children = {}\n",
    "        \n",
    "        return\n",
    "    \n",
    "    def value(self):\n",
    "        if self.visit_count == 0:\n",
    "            return 0\n",
    "        return self.value_sum/self.visit_count\n",
    "    \n",
    "    def select_action(self, temperature=1):\n",
    "        \"\"\"\n",
    "        Select action according to the visit count distribution and the temperature.\n",
    "        \"\"\"\n",
    "        visit_counts = np.array([child.visit_count for child in self.children.values()])\n",
    "        actions = [action for action in self.children.keys()]\n",
    "        if temperature == 0:\n",
    "            action = actions[np.argmax(visit_counts)]\n",
    "        elif temperature == float(\"inf\"):\n",
    "            action = np.random.choice(actions)\n",
    "        else:\n",
    "            # See paper appendix Data Generation\n",
    "            visit_count_distribution = visit_counts ** (1 / temperature)\n",
    "            visit_count_distribution = visit_count_distribution / sum(visit_count_distribution)\n",
    "            action = np.random.choice(actions, p=visit_count_distribution)\n",
    "\n",
    "        return action\n",
    "\n",
    "    def select_child(self):\n",
    "        \"\"\"\n",
    "        Select the child with the highest UCB score.\n",
    "        \"\"\"\n",
    "        best_score = -np.inf\n",
    "        best_action = -1\n",
    "        best_child = None\n",
    "\n",
    "        for action, child in self.children.items():\n",
    "            score = ucb_score(self, child)\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_action = action\n",
    "                best_child = child\n",
    "\n",
    "        return best_action, best_child\n",
    "\n",
    "    \n",
    "    def expand(self,action_probs):\n",
    "        for i,a in enumerate(action_probs):\n",
    "            if(a!=0):\n",
    "                next_state = self.state.copy()\n",
    "                \n",
    "                j = i//game.m\n",
    "                k = i%game.n\n",
    "                next_state[j][k] = self.player\n",
    "\n",
    "                self.children[i] = Node(a,next_state, self.player*(-1))\n",
    "            \n",
    "    def __repr__(self):\n",
    "        return str(self.value())+\"/\"+str(self.visit_count)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mcts(root, simulations=5):\n",
    "    for i in range(simulations):\n",
    "        leaf = root\n",
    "        search_path = [leaf]\n",
    "        \n",
    "        while leaf.children:\n",
    "            action, leaf = leaf.select_child()\n",
    "            search_path.append(leaf)\n",
    "        \n",
    "        game.board = leaf.state\n",
    "        game.player = leaf.player\n",
    "               \n",
    "        status, value = game.check_victory()\n",
    "        value = -1*abs(value)\n",
    "        \n",
    "        if not status:\n",
    "            inputs = game.get_input_planes()\n",
    "            inputs = tf.expand_dims(inputs,0)\n",
    "            \n",
    "            action_probs, value = model_wrap(inputs)\n",
    "            \n",
    "            action_probs = np.array(action_probs[0])\n",
    "            \n",
    "            action_probs = action_probs * game.mask_actions()\n",
    "            action_probs = action_probs/np.sum(action_probs)\n",
    "            \n",
    "            leaf.expand(action_probs)\n",
    "        \n",
    "        game.board = root.state\n",
    "        game.player = root.player\n",
    "        \n",
    "        #backpropagating values\n",
    "        for node in search_path[::-1]:\n",
    "            node.value_sum += float(value) if node.player == leaf.player else -float(value)\n",
    "            node.visit_count += 1\n",
    "  \n",
    "    return root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode_mcts(runs=1,simulations_per_move=5,temperature=1,q=1,z=0): \n",
    "    boards = []\n",
    "    action_probs = []\n",
    "    rewards_q = [] #uses the MCTS Q-value for each move\n",
    "    rewards_z = [] #0 for tie, -1 for losing and +1 for winning\n",
    "    #q=1 #rewards_q weight\n",
    "    #z=0 #rewardz_z weight\n",
    "    \n",
    "    for i in range(runs):\n",
    "        game.reset()\n",
    "        \n",
    "        #print('.',end=\"\")\n",
    "        root = Node(0,game.board,game.player)\n",
    "        \n",
    "        size = game.m*game.n\n",
    "        \n",
    "        for j in range(size):\n",
    "            root = mcts(root, simulations_per_move)\n",
    "            \n",
    "            action = root.select_action(temperature)\n",
    "\n",
    "            action_probs_t = np.zeros(size)\n",
    "            for k, v in root.children.items():\n",
    "                action_probs_t[k] = v.visit_count\n",
    "            action_probs_t = action_probs_t / np.sum(action_probs_t)\n",
    "\n",
    "            action_probs.append(action_probs_t)\n",
    "            rewards_q.append(root.value())\n",
    "            boards.append(game.get_input_planes())\n",
    "\n",
    "            root = root.children[action]\n",
    "            game.do_action(action)\n",
    "\n",
    "            status, winner = game.check_victory()\n",
    "            if status:\n",
    "                break\n",
    "        \n",
    "        reward = [winner*(-1)**x for x in range(j+1)]\n",
    "        rewards_z.extend(reward)\n",
    "      \n",
    "    boards = np.array(boards, dtype = \"float32\")\n",
    "    action_probs = np.array(action_probs, dtype = \"float32\")\n",
    "    rewards_q = np.array(rewards_q, dtype = \"float32\")\n",
    "    rewards_z = np.array(rewards_z, dtype = \"float32\")\n",
    "    \n",
    "    rewards = q*rewards_q + z*rewards_z\n",
    "    return boards, action_probs, rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235 ms ± 27.4 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "run_episode_mcts(1,20,float(\"inf\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QbQfJnv6dTMM"
   },
   "source": [
    "# Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "2gutxBSLdSt0"
   },
   "outputs": [],
   "source": [
    "huber_loss = tf.keras.losses.Huber(reduction=tf.keras.losses.Reduction.SUM)\n",
    "@tf.function\n",
    "def compute_loss(action_probs, reward, policy, values):\n",
    "    \n",
    "    actor_loss = tf.reduce_mean(-(action_probs*tf.math.log(policy)))\n",
    "    \n",
    "    critic_loss =  tf.reduce_mean(huber_loss(reward, values))\n",
    "\n",
    "    return actor_loss + critic_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JwGATaBlvVlC"
   },
   "source": [
    "# Train Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate = 0.003)\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "@tf.function\n",
    "def train_step_mcts(boards, action_probs, reward):\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        # Calculating \n",
    "        policy, value = model(boards)\n",
    "\n",
    "        # Calculating loss values to update our network\n",
    "        loss = compute_loss(action_probs, reward, policy, value)\n",
    "\n",
    "    grads = tape.gradient(loss,model.trainable_variables)\n",
    "    grads = tf.clip_by_global_norm(grads,5)[0]\n",
    "    optimizer.apply_gradients(zip(grads,model.trainable_variables))\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "n2CpgKJ1MOdP"
   },
   "outputs": [],
   "source": [
    "class rand():\n",
    "    def __init__(self):\n",
    "        return\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"random actor\"\n",
    "    \n",
    "    def select_action(self):\n",
    "        moves = game.valid_actions()\n",
    "        return np.random.choice(moves)\n",
    "\n",
    "    \n",
    "class pred():\n",
    "    def __init__(self,exploit=True):\n",
    "        self.exploit = exploit\n",
    "        return \n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"single-shot prediction\"\n",
    "    \n",
    "    def select_action(self):\n",
    "        inputs = game.get_input_planes()\n",
    "        inputs = tf.expand_dims(inputs,0)\n",
    "        \n",
    "        action_probs,_ = model_wrap(inputs)\n",
    "\n",
    "        valid_probs = action_probs * game.mask_actions()\n",
    "        valid_probs = valid_probs/tf.reduce_sum(valid_probs)\n",
    "\n",
    "        if(self.exploit):\n",
    "            action = np.argmax(valid_probs)\n",
    "        else:\n",
    "            action = np.random.choice(list(range(len(valid_probs))),p=valid_probs)\n",
    "\n",
    "        return action\n",
    "\n",
    "class perf():\n",
    "    ###Almost-perfect player for tic-tac-toe\n",
    "\n",
    "    def __init__(self):\n",
    "        return\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"game-informed actor\"\n",
    "    \n",
    "    def select_action(self):\n",
    "        assert game.m == 3 and game.n==3 and game.k==3\n",
    "        \n",
    "        b = game.board.flatten()\n",
    "        player = game.player\n",
    "        pos = [[0,1,2], [3,4,5], [6,7,8], [0,3,6], [1,4,7], [2,5,8], [0,4,8], [2,4,6]]\n",
    "\n",
    "        for x in pos:\n",
    "            if(np.sum(b[x])==player*2):\n",
    "                for y in x:\n",
    "                    if(b[y]==0):\n",
    "                        return y\n",
    "        for x in pos:\n",
    "            if(np.sum(b[x])==player*(-2)):\n",
    "                for y in x:\n",
    "                    if(b[y]==0):\n",
    "                        return y\n",
    "\n",
    "        if(b[4]==0):\n",
    "            return 4\n",
    "    \n",
    "        actions = game.valid_actions()\n",
    "        corners = list(set(actions).intersection({0,2,6,8}))\n",
    "        if(corners):\n",
    "            return np.random.choice(corners)\n",
    "        return np.random.choice(actions)\n",
    "    \n",
    "    \n",
    "class mcts_agent():\n",
    "    def __init__(self,simulations,temperature=0):\n",
    "        self.root = Node(0,game.board,game.player)\n",
    "        self.simulations = simulations\n",
    "        self.temperature = temperature\n",
    "        return\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"MCTS(\"+str(self.simulations)+\",\"+str(self.temperature)+\") actor\"\n",
    "    \n",
    "    def select_action(self):\n",
    "        if(self.root and self.root.children):\n",
    "            self.root = self.root.children[game.history[-1]]\n",
    "        else:\n",
    "            self.root = Node(0,game.board,game.player)   \n",
    "        self.root = mcts(self.root, self.simulations)\n",
    "        action = self.root.select_action(self.temperature)\n",
    "        self.root = self.root.children[action]\n",
    "        return action\n",
    "    \n",
    "class human():\n",
    "    def _init_(self):\n",
    "        return\n",
    "    \n",
    "    def select_action(self):\n",
    "        clear_output()\n",
    "        print(\"Current board\")\n",
    "        print(game)\n",
    "        print(\"Available moves\")\n",
    "        print(game.valid_actions())\n",
    "        action = input()\n",
    "        return int(action)\n",
    "    \n",
    "def single_game(p1,p2,verbose=False):\n",
    "    game.reset()\n",
    "    players = [p1,p2]\n",
    "    p = 0\n",
    "    \n",
    "    while not game.check_victory()[0]:\n",
    "        if(verbose):\n",
    "            print(\".\")\n",
    "            print(game)\n",
    "        move = players[p].select_action()\n",
    "        game.do_action(move)\n",
    "\n",
    "        p = 1 - p\n",
    "\n",
    "    if(verbose):\n",
    "        print(\".\")\n",
    "        print(game)\n",
    "    \n",
    "    return game.check_victory()[1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t9-o3qDBh0SY"
   },
   "source": [
    "# Train Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_episode(max_episodes,runs,simulations,temperature,q,z):\n",
    "    print(\"\\nStarting episode batch\")\n",
    "    for i in range(1,max_episodes+1):\n",
    "        boards, action_probs, reward = run_episode_mcts(runs,simulations,temperature,q,z)\n",
    "        j = 0\n",
    "\n",
    "        examples_size = len(reward)\n",
    "        while j < examples_size/BATCH_SIZE:\n",
    "            idx = np.random.randint(examples_size, size=BATCH_SIZE)\n",
    "            train_step_mcts(boards[idx], action_probs[idx], reward[idx])\n",
    "            j=j+1\n",
    "\n",
    "        print(\".\",end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "BO3A_ddNMlkG"
   },
   "outputs": [],
   "source": [
    "def test(players,runs):\n",
    "    res = np.zeros([2,3])\n",
    "    def _test():\n",
    "        print(\"Testing\")\n",
    "        print(\"P1:\",p1,\" vs. \",\"P2:\", p2)\n",
    "        for k in range(runs):\n",
    "            p1.root = Node(0,game.board,game.player)\n",
    "            p2.root = Node(0,game.board,game.player)\n",
    "\n",
    "            winner = int(single_game(p1, p2))\n",
    "            res[i][winner] += 1\n",
    "        print(\"Ties/P1 wins/P2 wins:\",res[i])\n",
    "    \n",
    "    p1 = players[0]\n",
    "    p2 = players[1]\n",
    "    i=0\n",
    "    _test()\n",
    "    \n",
    "    p1 = players[1]\n",
    "    p2 = players[0]\n",
    "    i=1\n",
    "    _test()\n",
    "        \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "quz52ezAhz2y",
    "outputId": "e3439d04-1d53-4684-dfde-82b7de100871"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting episode batch\n",
      "..................................................Testing\n",
      "P1: single-shot prediction  vs.  P2: random actor\n",
      "Ties/P1 wins/P2 wins: [154. 632. 214.]\n",
      "Testing\n",
      "P1: random actor  vs.  P2: single-shot prediction\n",
      "Ties/P1 wins/P2 wins: [ 31. 480. 489.]\n",
      "\n",
      "Starting episode batch\n",
      "..................................................Testing\n",
      "P1: single-shot prediction  vs.  P2: random actor\n",
      "Ties/P1 wins/P2 wins: [139. 663. 198.]\n",
      "Testing\n",
      "P1: random actor  vs.  P2: single-shot prediction\n",
      "Ties/P1 wins/P2 wins: [ 31. 493. 476.]\n",
      "\n",
      "Starting episode batch\n",
      "..................................................Testing\n",
      "P1: single-shot prediction  vs.  P2: random actor\n",
      "Ties/P1 wins/P2 wins: [143. 649. 208.]\n",
      "Testing\n",
      "P1: random actor  vs.  P2: single-shot prediction\n",
      "Ties/P1 wins/P2 wins: [ 28. 529. 443.]\n",
      "\n",
      "Starting episode batch\n",
      "..................................................Testing\n",
      "P1: single-shot prediction  vs.  P2: random actor\n",
      "Ties/P1 wins/P2 wins: [152. 670. 178.]\n",
      "Testing\n",
      "P1: random actor  vs.  P2: single-shot prediction\n",
      "Ties/P1 wins/P2 wins: [ 26. 508. 466.]\n",
      "Wall time: 9min 41s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[152., 670., 178.],\n",
       "       [ 26., 508., 466.]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "#simulations batch_size temperature learning_rate episodes model_struct q&z \n",
    "max_episodes = 50\n",
    "model = combined_model(1)\n",
    "\n",
    "train_episode(max_episodes,10,20,float(\"inf\"),1,5)\n",
    "test([pred(),rand()],1000)\n",
    "\n",
    "train_episode(max_episodes,10,20,10,2,4)\n",
    "test([pred(),rand()],1000)\n",
    "\n",
    "train_episode(max_episodes,10,20,1,4,2)\n",
    "test([pred(),rand()],1000)\n",
    "\n",
    "train_episode(max_episodes,10,20,0.1,5,1)\n",
    "test([pred(),rand()],1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Y78-glrMpBZ"
   },
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing\n",
      "P1: single-shot prediction  vs.  P2: game-informed actor\n",
      "Ties/P1 wins/P2 wins: [  0.   0. 100.]\n",
      "Testing\n",
      "P1: game-informed actor  vs.  P2: single-shot prediction\n",
      "Ties/P1 wins/P2 wins: [  0. 100.   0.]\n",
      "Wall time: 1.88 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[  0.,   0., 100.],\n",
       "       [  0., 100.,   0.]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "test([pred(),perf()],100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing\n",
      "P1: MCTS(5,0) actor  vs.  P2: game-informed actor\n",
      "Ties/P1 wins/P2 wins: [  0.   0. 100.]\n",
      "Testing\n",
      "P1: game-informed actor  vs.  P2: MCTS(5,0) actor\n",
      "Ties/P1 wins/P2 wins: [  0. 100.   0.]\n",
      "Wall time: 6.98 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[  0.,   0., 100.],\n",
       "       [  0., 100.,   0.]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "test([mcts_agent(simulations = 5),perf()],100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing\n",
      "P1: MCTS(5,0) actor  vs.  P2: game-informed actor\n",
      "Ties/P1 wins/P2 wins: [  0.   0. 100.]\n",
      "Testing\n",
      "P1: game-informed actor  vs.  P2: MCTS(5,0) actor\n",
      "Ties/P1 wins/P2 wins: [  0. 100.   0.]\n",
      "Wall time: 7.06 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[  0.,   0., 100.],\n",
       "       [  0., 100.,   0.]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "test([mcts_agent(simulations = 5),perf()],100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing\n",
      "P1: MCTS(10,0) actor  vs.  P2: game-informed actor\n",
      "Ties/P1 wins/P2 wins: [64.  0. 36.]\n",
      "Testing\n",
      "P1: game-informed actor  vs.  P2: MCTS(10,0) actor\n",
      "Ties/P1 wins/P2 wins: [  0. 100.   0.]\n",
      "Wall time: 15 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 64.,   0.,  36.],\n",
       "       [  0., 100.,   0.]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "test([mcts_agent(simulations = 10),perf()],100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing\n",
      "P1: MCTS(20,0) actor  vs.  P2: game-informed actor\n",
      "Ties/P1 wins/P2 wins: [100.   0.   0.]\n",
      "Testing\n",
      "P1: game-informed actor  vs.  P2: MCTS(20,0) actor\n",
      "Ties/P1 wins/P2 wins: [44. 56.  0.]\n",
      "Wall time: 28.2 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[100.,   0.,   0.],\n",
       "       [ 44.,  56.,   0.]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "test([mcts_agent(simulations = 20),perf()],100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing\n",
      "P1: MCTS(50,0) actor  vs.  P2: game-informed actor\n",
      "Ties/P1 wins/P2 wins: [31. 33. 36.]\n",
      "Testing\n",
      "P1: game-informed actor  vs.  P2: MCTS(50,0) actor\n",
      "Ties/P1 wins/P2 wins: [27. 73.  0.]\n",
      "Wall time: 58.5 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[31., 33., 36.],\n",
       "       [27., 73.,  0.]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "test([mcts_agent(simulations=50),perf()],100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing\n",
      "P1: game-informed actor  vs.  P2: random actor\n",
      "Ties/P1 wins/P2 wins: [ 410. 9590.    0.]\n",
      "Testing\n",
      "P1: random actor  vs.  P2: game-informed actor\n",
      "Ties/P1 wins/P2 wins: [1352.  128. 8520.]\n",
      "Wall time: 22.2 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 410., 9590.,    0.],\n",
       "       [1352.,  128., 8520.]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "test([perf(),rand()],10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing\n",
      "P1: MCTS(20,0) actor  vs.  P2: random actor\n",
      "Ties/P1 wins/P2 wins: [ 54. 894.  52.]\n",
      "Testing\n",
      "P1: random actor  vs.  P2: MCTS(20,0) actor\n",
      "Ties/P1 wins/P2 wins: [295. 201. 504.]\n",
      "Wall time: 4min 7s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 54., 894.,  52.],\n",
       "       [295., 201., 504.]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "test([mcts_agent(simulations = 20),rand()],1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "reinforcement_learning.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
